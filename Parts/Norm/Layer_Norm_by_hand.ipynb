{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "相比于 Batch Norm，Layer Norm (LN) 其实更好写，逻辑也更简单。\n",
    "\n",
    "因为它不需要维护 Running Mean/Var，训练和推理的逻辑是完全一样的。"
   ],
   "id": "2ff17fdc055141a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T05:26:54.620840Z",
     "start_time": "2026-01-19T05:26:52.819846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ],
   "id": "112fcbba136f2a3b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T06:10:02.713758Z",
     "start_time": "2026-01-19T06:10:02.692260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super(MyLayerNorm, self).__init__()\n",
    "\n",
    "        if isinstance(normalized_shape, int):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "\n",
    "        self.eps = eps\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "        #1 即可学习参数\n",
    "        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(normalized_shape))\n",
    "\n",
    "        # 注意：LayerNorm 不需要 register_buffer 存 running_mean/var！\n",
    "\n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        :param x: [B, Seq_len, Hidden_Dim]\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        # 1. 确定要在哪些维度上求均值和方差\n",
    "        # 如果 normalized_shape 是 (D,), 那么就是对倒数第 1 个维度求均值\n",
    "        # 如果 normalized_shape 是 (H, W), 那么就是对倒数第 2, 1 个维度求均值\n",
    "        dims = tuple(range(len(x.shape) - len(self.normalized_shape), len(x.shape)))\n",
    "\n",
    "        #计算均值和方差\n",
    "        mean = x.mean(dim=dims, keepdim=True)\n",
    "        var = x.var(dim=dims, keepdim=True,unbiased=False)\n",
    "\n",
    "        #3 归一化\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        # 4. 仿射变换\n",
    "        # PyTorch 会自动广播 gamma/beta 到 x_norm 的形状\n",
    "        out = self.gamma * x_norm + self.beta\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "ee4eb79b5f4470b9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "验证代码",
   "id": "54149e51d5ae570e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T06:10:05.044303Z",
     "start_time": "2026-01-19T06:10:04.941111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 验证代码 ---\n",
    "# 模拟 Transformer 输入: Batch=2, Seq_Len=5, Dim=4\n",
    "x = torch.randn(2, 5, 4)\n",
    "\n",
    "# 实例化\n",
    "dim = 4\n",
    "my_ln = MyLayerNorm(dim)\n",
    "torch_ln = nn.LayerNorm(dim)\n",
    "\n",
    "# 统一权重\n",
    "torch_ln.weight.data = my_ln.gamma.data\n",
    "torch_ln.bias.data = my_ln.beta.data\n",
    "\n",
    "# 验证输出\n",
    "out_my = my_ln(x)\n",
    "out_torch = torch_ln(x)\n",
    "\n",
    "print(\"LayerNorm 误差:\", (out_my - out_torch).abs().max().item())"
   ],
   "id": "e4558ca4d8537076",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm 误差: 2.384185791015625e-07\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
